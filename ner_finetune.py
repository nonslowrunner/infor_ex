# -*- coding: utf-8 -*-
"""ner_finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GXEMGbQkMHSvF1B6FgGzXEHiPxRnTOGF

# GPU SET UP
"""



gpu_info = !nvidia-smi
gpu_info = '\n'.join(gpu_info)
if gpu_info.find('failed') >= 0:
  print('Select the Runtime > "Change runtime type" menu to enable a GPU accelerator, ')
  print('and then re-execute this cell.')
else:
  print(gpu_info)


!git lfs install
!git clone https://huggingface.co/airesearch/wangchanberta-base-att-spm-uncased
# if you want to clone without large files – just their pointers
# prepend your git clone with the following env var:
!GIT_LFS_SKIP_SMUDGE=1

!git lfs install
!git clone https://huggingface.co/airesearch/xlm-roberta-base-finetuned
# if you want to clone without large files – just their pointers
# prepend your git clone with the following env var:
!GIT_LFS_SKIP_SMUDGE=1

"""# Dependency """

!pip install -q git+git://github.com/huggingface/datasets.git@0281f9d881f3a55c89aeaa642f1ba23444b64083
!Clone thai2transformer from`dev` branch
!rm -rf thai2transformers
!git clone -b dev https://github.com/vistec-AI/thai2transformers.git

!pip install -q datasets
!pip install -q thai2transformers
!pip install -q torch==1.5
!pip install --upgrade datasets

"""# Preprocessing data"""

import sys

sys.path.append('./thai2transformers')
sys.path.append('./thai2transformers/scripts')
sys.path

import tensorflow as tf

from datasets import load_dataset, load_metric, Dataset
from thai2transformers.metrics import classification_metrics
from sklearn.metrics import f1_score
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
from functools import lru_cache
import transformers

from functools import lru_cache
from seqeval.metrics import classification_report
from sklearn.metrics import classification_report as sk_classification_report
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# thai2transformers
from thai2transformers import metrics as t2f_metrics
from thai2transformers.tokenizers import (
    ThaiRobertaTokenizer, ThaiWordsNewmmTokenizer,
    ThaiWordsSyllableTokenizer, FakeSefrCutTokenizer,
    SPACE_TOKEN as DEFAULT_SPACE_TOKEN, SEFR_SPLIT_TOKEN)

from transformers import (Trainer, TrainingArguments,
                          AutoModelForTokenClassification, AutoTokenizer,
                          HfArgumentParser, CamembertTokenizer , TFTrainingArguments , TFTrainer , TFCamembertForTokenClassification)

import logging
logger = logging.getLogger(__name__)
from custom_data_collator import DataCollatorForTokenClassification

import pickle  
import re
from tqdm import tqdm
import datetime, os

from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer , CamembertForTokenClassification , TFAutoModelForTokenClassification , pipeline

from datasets import load_dataset

dataset = load_dataset("thainer")

label_col = 'ner_tags'
dataset['train'] = dataset['train'].map(
lambda examples: {'ner_tags': [i if i not in [13, 26] else 27
                                for i in examples[label_col]]}
)
label_maps = {i: name for i, name in
      enumerate(dataset['train'].features[label_col].feature.names)}
      
label_names = dataset['train'].features[label_col].feature.names
num_labels = dataset['train'].features[label_col].feature.num_classes

from datasets import load_dataset

dataset = load_dataset("lst20", data_dir="/content/drive/MyDrive/dataset/lst20_zip/LST20_Corpus")
# dataset = load_dataset("lst20", data_dir="./LST20_Corpus")

#parameters
class Args:
    dataset_name_or_path = 'lst20'
    feature_col = 'tokens'
    label_col = 'ner_tags'
    metric_for_best_model = 'f1_macro'
    seed = 2020
    data_dir = '/content/drive/MyDrive/dataset/lst20_zip/LST20_Corpus'
    # data_dir = './LST20_Corpus'

args = Args()

train_valtest_split = dataset['train'].train_test_split(test_size=0.2, shuffle=True, seed=args.seed)
dataset['train'] = train_valtest_split['train']
dataset['validation'] = train_valtest_split['test']
val_test_split = dataset['validation'].train_test_split(test_size=0.5, shuffle=True, seed=args.seed)
dataset['validation'] = val_test_split['train']
dataset['test'] = val_test_split['test']
 
num_labels = dataset['train'].features['pos_tags'].feature.num_classes

dataset

label_list = dataset['train'].features['ner_tags'].feature.names

len(label_list)

import json

id2label = {  label:i for i,label in enumerate(label_list)}
id2label = {  str(i) : label for i,label in enumerate(label_list)}

y = json.dumps(id2label)

y

from transformers import AutoTokenizer , CamembertTokenizer, RobertaForTokenClassification , TFAutoModel , AutoModel
from thai2transformers.tokenizers import (
    ThaiRobertaTokenizer, ThaiWordsNewmmTokenizer)

# tokenizer = CamembertTokenizer.from_pretrained('./drive/MyDrive/BERT/core_model')
# tokenizer.additional_special_tokens = ['<s>NOTUSED', '</s>NOTUSED', '<_>']

tokenizer = ThaiWordsNewmmTokenizer.from_pretrained('./drive/MyDrive/BERT/core_newmm')

def pre_tokenize(token):
    token = ''.join(token)
    token = token.replace('<_>' , ' ')
    return token


def is_not_too_long(example,
                    max_length=128):
    tokens = len(tokenizer.tokenize(pre_tokenize(example)))
    return tokens < max_length

dataset['train'] = dataset['train'].filter(is_not_too_long)
dataset['test'] = dataset['test'].filter(is_not_too_long)
dataset['validation'] = val_test_split['train']
dataset['validation'] = dataset['validation'].filter(is_not_too_long)

# from transformers import (Trainer, TrainingArguments,
#                           AutoModelForTokenClassification, AutoTokenizer,
#                           HfArgumentParser, CamembertTokenizer , TFAutoModelForTokenClassification)
# model = AutoModelForTokenClassification.from_pretrained("./wangchanberta-base-att-spm-uncased", num_labels=num_labels)
# model = TFAutoModelForTokenClassification.from_pretrained('./drive/MyDrive/BERT/core_model' , num_labels=16)

text_col =   'tokens'
label_col  = 'ner_tags'

@lru_cache(maxsize=None)
def cached_tokenize(token, space_token='<_>',
                    lowercase=False):
    if lowercase:
        token = token.lower()
    token = pre_tokenize(token)
    ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(token))
    return ids

cached_tokenize = cached_tokenize

def preprocess(examples, space_token='<_>', lowercase= False):
    tokens = []
    labels = []
    old_positions = []
    for example_tokens, example_labels in zip(examples[text_col], examples[label_col]):
        new_example_tokens = []
        new_example_labels = []
        old_position = []
        for i, (token, label) in enumerate(zip(example_tokens, example_labels)):
            # tokenize each already pretokenized tokens with our own tokenizer.
            toks = cached_tokenize(token, '<_>', lowercase=False)
            n_toks = len(toks)
            new_example_tokens.extend(toks)
            # expand label to cover all tokens that get split in a pretokenized token
            new_example_labels.extend([label] * n_toks)
            # kept track of old position
            old_position.extend([i] * n_toks)
        tokens.append(new_example_tokens)
        labels.append(new_example_labels)
        old_positions.append(old_position)
    tokenized_inputs = tokenizer._batch_prepare_for_model(
        [(e, None) for e in tokens],
        truncation_strategy=transformers.tokenization_utils_base.TruncationStrategy.LONGEST_FIRST,
        add_special_tokens=True, max_length=128)
    # in case of needed truncation we need to chop off some of the labels manually
    max_length = max(len(e) for e in tokenized_inputs['input_ids'])
    # add -100 to first and last token which is special tokens for <s> and </s>
    # -100 is a convention for padding in higgingface transformer lib
    # and calculating loss should skip this
    tokenized_inputs['old_positions'] = [[-100] + e[:max_length - 2] + [-100]
                                         for e in old_positions]
    tokenized_inputs['labels'] = [[-100] + e[:max_length - 2] + [-100]
                                  for e in labels]
    return tokenized_inputs

data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)

train_dataset = dataset['train']
val_dataset = dataset['validation']
test_dataset = dataset['test']

train_dataset = Dataset.from_dict(preprocess(train_dataset))
val_dataset = Dataset.from_dict(preprocess(val_dataset))
test_dataset = Dataset.from_dict(preprocess(test_dataset))

from datasets import load_dataset, load_metric
metric = load_metric("seqeval")

def t2t_sk_classification_metrics(agg_chunk_labels, agg_chunk_preds):
    class LabelsPreds:
        label_ids = agg_chunk_labels
        predictions = agg_chunk_preds
    return t2f_metrics.sk_classification_metrics(LabelsPreds, pred_labs=True)

def compute_metrics(p):
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_maps[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_maps[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    if 'ner' in data_args.label_name:
        results = metric.compute(predictions=true_predictions, references=true_labels)
        return {
            "precision": results["overall_precision"],
            "recall": results["overall_recall"],
            "f1": results["overall_f1"],
            "accuracy": results["overall_accuracy"],
        }
    else:
        result = t2t_sk_classification_metrics(sum(true_labels, []),
                                               sum(true_predictions, []))
        result = {k: v for k, v in result.items() if k != 'classification_report'}
        return result

# model = AutoModelForTokenClassification.from_pretrained('./drive/MyDrive/BERT/core_model' , num_labels=num_labels  , from_tf=True)
# model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/BERT/core_newmm/tf' , num_labels=len(label_list) , from_tf=True)
model = CamembertForTokenClassification.from_pretrained("./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20/")

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard
# %tensorboard --logdir ./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20/logs

training_args = TrainingArguments(
    output_dir='./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20',          # output directory
    num_train_epochs=40,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=100,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir="./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20/logs",            # directory for storing logs
    logging_steps=10,
    save_steps=5000,
    save_total_limit=1
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.save_model()

tokenizer = ThaiWordsNewmmTokenizer.from_pretrained('./drive/MyDrive/BERT/core_newmm')

tagger_pos_group = pipeline(task='ner',
         tokenizer= tokenizer,
         model = CamembertForTokenClassification.from_pretrained("./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20/"),
         ignore_labels=[], 
         grouped_entities=True)

tagger_pos_group_no = pipeline(task='ner',
         tokenizer= tokenizer,
         model = CamembertForTokenClassification.from_pretrained("./drive/MyDrive/BERT/fine_tuning/ner/finetune_ner_newmm_lst20/"),
         ignore_labels=[], 
         grouped_entities=False)

doc = ['สัญญาฉบับนี้ทำขึ้น   ณ   สถานพินิจและคุ้มครองเด็กและเยาวชนจังหวัดหนองคาย         ถนนมีชัย  อำเภอเมืองหนองคาย  จังหวัดหนองคาย  เมื่อวันที่   1  ตุลาคม  2559  ระหว่างกรมพินิจและคุ้มครองเด็กและเยาวชน  โดย  นางนันทนา พันธุ์ชื่น ผู้อำนวยการสถานพินิจและคุ้มครองเด็กและเยาวชนจังหวัดหนองคาย  ผู้รับมอบอำนาจจากอธิบดีกรมพินิจและคุ้มครองเด็กและเยาวชน  ตามคำสั่งที่  127/2559      ลงวันที่  19 กุมภาพันธ์ 2559  ลำดับที่ 5.2 การเช่า มอบให้มีอำนาจต่อสัญญาเช่าสังหาริมทรัพย์และต่อสัญญาเช่าอสังหาริมทรัพย์ สำหรับหน่วยงานที่กรมได้อนุมัติให้เช่าหรือดำเนินการเช่าไว้ก่อนแล้วแต่กำหนดเวลาตามสัญญาเช่าได้สิ้นสุดลง และมีความจำเป็นต้องเช่าอย่างต่อเนื่อง โดยไม่มีการเปลี่ยนแปลงวงเงิน เงื่อนไขการเช่า หรือข้อกำหนดตามสัญญาเดิม ซึ่งต่อไป  ในสัญญานี้เรียกว่า“ผู้เช่า” ฝ่ายหนึ่ง  กับ  นางอาภรณ์สวัสดิ์             พลศักดิ์  บัตรประจำตัวประชาชน  เลขที่  3 4399 00038 58 5  อยู่บ้านเลขที่ 897  ถนนมีชัย ตำบลในเมือง อำเภอเมืองหนองคาย  จังหวัดหนองคาย    ซึ่งต่อไปในสัญญานี้เรียกว่า  “ผู้ให้เช่า”   อีกฝ่ายหนึ่ง',
'เนื่องจากผู้เช่ามีความประสงค์จะเช่าอาคารพาณิชย์   3   ชั้น   จำนวน   3  คูหา                 ตั้งอยู่เลขที่  897/10-12   ถนนมีชัย  ตำบลในเมือง  อำเภอเมืองหนองคาย  จังหวัดหนองคาย  เพื่อใช้เป็นที่ทำการสถานพินิจและคุ้มครองเด็กและเยาวชนจังหวัดหนองคาย  คู่สัญญาทั้งสองฝ่ายจึงได้ทำความตกลงกันมีข้อความดังต่อไปนี้',
'ส่วนที่ 1',
'บททั่วไป',
'ข้อ 1 ทรัพย์สินที่เช่า',
'ผู้ให้เช่าตกลงให้เช่า และผู้เช่าตกลงเช่าอาคารพาณิชย์ 3 ชั้น จำนวน 3 คูหา ตั้งอยู่เลขที่  897/10-12  ถนนมีชัย  ตำบลในเมือง  อำเภอเมืองหนองคาย  จังหวัดหนองคาย  ตลอดจนที่ดินและสิ่งอำนวยความสะดวกที่ใช้ประโยชน์เกี่ยวเนื่องกับอาคารดังกล่าว ซึ่งต่อไปในสัญญานี้เรียกว่า “ทรัพย์สินที่เช่า”'
'ข้อ 2  กรรมสิทธิ์ในทรัพย์สินที่เช่า'
,'ผู้ให้เช่ารับรองว่าทรัพย์สินที่เช่าเป็นกรรมสิทธิ์ของผู้ให้เช่า ซึ่งผู้ให้เช่ามีสิทธิ์นำออกให้เช่าได้'
,'ข้อ 3  การส่งมอบทรัพย์สินที่เช่า'
,'ผู้ให้เช่าสามารถส่งมอบทรัพย์สินที่เช่าให้แก่ผู้เช่าได้ในวันที่  1  ตุลาคม  2559  และติดตั้งระบบไฟฟ้า ระบบน้ำประปา ให้อยู่ในสภาพที่พร้อมที่จะใช้งานได้ดี  กรณีผู้เช่ามีความประสงค์จะเข้าทำการปรับปรุงทรัพย์สินที่เช่าก่อนจะเข้าใช้ประโยชน์ในทรัพย์สินที่เช่า  ผู้ให้เช่าจะต้องส่งมอบทรัพย์สินที่เช่าให้ผู้เช่าทำการปรับปรุงนับแต่วันที่ลงนามในสัญญาหรือวันที่ได้รับแจ้งจากผู้เช่า  และผู้ให้เช่าจะอำนวยความสะดวก ต่าง ๆ  ในการเข้าทำการปรับปรุงดังกล่าว'
,'ข้อ 4  ระยะเวลาการเช่า'
,'ผู้ให้เช่าตกลงให้เช่าและผู้เช่าตกลงเช่าทรัพย์สินที่เช่ามีกำหนดระยะเวลา 12  เดือน                      นับแต่วันที่  1  ตุลาคม  2559  ถึงวันที่  30   กันยายน  2560'
,'ข้อ 5  ค่าเช่า'
,'		ผู้เช่าตกลงชำระค่าเช่าทรัพย์สินที่เช่าตามข้อ 1 ให้แก่ผู้ให้เช่าเป็นรายเดือนในอัตราค่าเช่า           เป็นเงินเดือนละ 22,500  บาท (สองหมื่นสองพันห้าร้อยบาทถ้วน) โดยผู้เช่าจะชำระค่าเช่าภายในวันที่ 15 ของเดือนถัดไป  หรือตามระเบียบของทางราชการ ค่าเช่านี้ได้รวมค่าภาษีมูลค่าเพิ่มและค่าภาษีอากรอื่น ๆ ด้วยแล้ว'
,'		กรณีการเช่าทรัพย์สินที่เช่าตามวรรคแรกไม่ครบเดือนแห่งปฏิทิน ให้คำนวณค่าเช่าต่อวัน          จากอัตราค่าเช่าต่อเดือนหารด้วย  30  (สามสิบ)'
,'ข้อ 6  คำมั่นจะให้เช่า'
,'เมื่อครบกำหนดระยะเวลาเช่าตามสัญญาฉบับนี้แล้ว  หากผู้เช่าประสงค์จะเช่าทรัพย์สินที่เช่าต่อไปอีก   ผู้ให้เช่าตกลงให้ผู้เช่า  เช่าต่อไปภายใต้เงื่อนไขการเช่าตามสัญญานี้ทุกประการ โดยผู้เช่าตกลงชำระค่าเช่าในสามปีถัดไปให้แก่ผู้ให้เช่าในอัตราค่าเช่าดังนี้'
,'		6.1  ตั้งแต่วันที่  1  ตุลาคม  2559  ถึงวันที่ 30  กันยายน 2560  อัตราค่าเช่าเดือนละ 22,500 บาท (สองหมื่นสองพันห้าร้อยบาทถ้วน)'
,'		6.2   ตั้งแต่วันที่  1  ตุลาคม  2560  ถึงวันที่ 30  กันยายน 2561  อัตราค่าเช่าเดือนละ 22,500 บาท (สองหมื่นสองพันห้าร้อยบาทถ้วน)'
,'6.3   ตั้งแต่วันที่  1  ตุลาคม  2561  ถึงวันที่ 30  กันยายน 2562  อัตราค่าเช่าเดือนละ 22,500 บาท (สองหมื่นสองพันห้าร้อยบาทถ้วน)'
,'		สำหรับค่าเช่าในปีต่อๆไป ทั้งสองฝ่ายจะทำความตกลงกันภายหลัง ทั้งนี้ ผู้เช่าจะต้องแจ้งความประสงค์จะเช่าต่อผู้ให้เช่าทราบเป็นลายลักษณ์อักษร   ก่อนกำหนดสิ้นสุดสัญญานี้ ไม่น้อยกว่า 30 วัน']

def clean(doc):
    if isinstance(doc, list):
        doc_clean = []
        for line in doc:
            line_clean = re.sub("[\n/.:,(){}<>*%$#@!_-]", '' , line)
            line_clean = re.sub('\s\s+', ' ', line_clean)
            doc_clean.append(line_clean)
    else:
            doc_clean = re.sub("[\n/.:,(){}<>*%$#@!_-]", '' , doc)
            doc_clean = re.sub('\s\s+', ' ', doc_clean)
            
    return doc_clean

def tagger(doc):
    enti = tagger_pos_group(doc)
    return [ (enti[i]['word'] , enti[i]['entity_group']) for i in range(len(enti))]

doc_contract = clean(doc)

tagger(doc_contract[10])

tagger('นายกรัฐมนตรี ดร.มหาธีร์ บินโมฮัมหมัด')

